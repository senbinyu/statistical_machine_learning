### SVM是什么？  
从分类平面，到求两类间的最大间隔，到转化为求间隔分之一的优化问题：loss=min(1/2·||W||·||W||) subject to：y(wx+b)>=1，其中||·||为2范数     
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题   
最后再利用SMO（序列最小优化）来解决这个对偶问题  

1. 什么叫最优超平面?  
用于分割特征空间不同类别的一个超平面，且每类数据离超平面最近的距离最大，则为最优超平面。来自百度百科

2. 什么是支持向量？  
https://zhuanlan.zhihu.com/p/106265377  
距离最优超平面的数据向量即为支持向量，是这些数据向量决定了最大分割间隔。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点。所以只有增多支持向量的数据，才能提高模型能力，否则无效。

3. SVM如何解决多分类问题？  
https://zhuanlan.zhihu.com/p/47895420  
一类是直接法，直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该最优化问题“一次性”实现多类分类。这种方法看似简单，但其计算复杂度比较高，实现起来比较困难，只适合用于小型问题中；另一类是间接法，主要是通过组合多个二分类器来实现多分类器的构造，常见的方法有one-against-one和one-against-all两种。若有k个类别，前者需要k(k-1)/2个,即C_k^2, 后者需要k个分类器。

4. SVM 能解决哪些问题？  
线性问题：对于n为数据，找到n-1维的超平面将数据分成2份。通过增加一个约束条件： 要求这个超平面到每边最近数据点的距离是最大的  
非线性问题：SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器

5. 什么叫软间隔？   
软间隔允许部分样本点不满足约束条件： y(wx+b) > 1

6. 介绍一下你知道的不同的SVM分类器？  
硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器  
软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器  
kernel SVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器  

7. 为什么要把原问题转换为对偶问题？  
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效。原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题。因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0。  
同时可以引入核函数  

8. 核函数的作用是什么？  
核函数能够将特征从低维空间映射到高维空间， 这个映射可以把低维空间中不可分的两类点变成高维线性可分的  
还有另外2点。1）核函数可以将高维降低到低维内积，从而实现计算简化；具体来讲，因为样本维数很高，容易造成“维数灾难”，所以这里我们就引入了核函数，把高维向量的内积转变成了求低维向量的内积问题。2）核函数中用到的两个向量内积，衡量的是两者的相似度，可以想象为衡量两个类的相似度。  
https://blog.csdn.net/jiangjieqazwsx/article/details/51418681

9. 核函数有哪些？  
![kernel_func](https://user-images.githubusercontent.com/42667259/91205766-5def3800-e706-11ea-912b-96da44a3a578.png)

10. Hinge loss，合页损失？   
Hinge loss（合页损失）用于训练分类器的损失函数，常被用于“最大间隔分类”。对于标签 t=±1，预测的结果y，其损失函数公式可以表示为：loss = max(0, 1-ty).当y落在满足条件的一侧时，不管是多少，损失函数为0；而在不满足条件的一侧，会逐渐增大。这就保证了普通向量损失为0，不参与超平面的最终决定，这才是SVM的核心所在，保证了SVM解的稀疏性。  
具体可见知乎链接：https://www.zhihu.com/question/47746939

