此处将linear regression和logistic regression放在一块。

## linear regression，线性回归
1. 吴恩达课程：损失函数，最小均方误差， MSE： J(w) = 1/2m sum_{i=1}^{m} (h_w(x_i) - y_i)^2  
<!--![lr_loss](https://user-images.githubusercontent.com/42667259/91022105-fc8b7400-e5f4-11ea-83c1-287feb35bbd3.png)-->

2. 梯度下降，因为h_w(x) = wx + b, w即为图中的\theta, 如下的\alpha即为学习率  
![lr_grad](https://user-images.githubusercontent.com/42667259/91022102-fbf2dd80-e5f4-11ea-88dc-c925ee985898.jpg)

3. 岭回归，意指在对角线上加正则项，状似岭，故名岭回归，其实就是加上正则化的线性回归。h_w(x) = h_w(x) + \lambda w_i^2

4. 什么时候使用岭回归，即什么时候使用正则化合适？  
当样本数少的时候，或者样本重复程度高。

5. 什么时候使用Lasso回归？  
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征

## logistic regression，逻辑斯蒂回归
1. 这是一种二分类方法，属于广义线性模型，logistic里用的是sigmoid函数，即y^ = h(wx+b),即在之前linear regression基础上加非线性函数sigmoid。
这里是二分类，（伯努利过程），p(y=1|x,w) = h(wx+b); p(y=0|x,w) = 1-h(wx+b)，p(y|x,θ) = h(wx+b)^y · (1-h(wx+b))^(1-y)，当所有的样本连乘，综合其发生的最大概率，即可将其看成求一个二分类分布的最大对数似然函数ylogy^ + (1-y)log(1-y^)，此处的y^=h(wx+b)，预测值，将其化为最小化损失函数，将上述取负号即可，如下图，  
![lr_loss2](https://user-images.githubusercontent.com/42667259/91023250-9acc0980-e5f6-11ea-8912-8fe65dbc34ff.jpg)  
以上就是CE损失。总的来讲可以这样理解，最大化对数似然函数，就是要最小化CE损失函数。

2. LR明明是分类模型为什么叫回归？  
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归

3. LR中的L1/L2正则项是什么？  
损失函数后面添加的正则项，也叫惩罚项，分为L1-norm, L2-norm  
J = J_0 + α∑∣w∣  
J = J_0 + α∑∣w∣^2  
另外，在sklearn的包中，也就是里面的penalty，惩罚项，即可选择L1, L2

4. Sigmoid函数到底起了什么作用？  
将数据压缩到[0, 1]范围内。  
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感。

5. 为什么不用MSE损失函数作为logistic regression的损失函数？  
mse下的logistic regression损失函数非凸，难以得到解析解

6. 为什么LR需要归一化或者取对数?  
模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大  
工程角度：加速收敛， 提高计算效率  
理论角度: 梯度下降过程稳定；使得数据在某类上更服从高斯分布，满足前提假设，这个是必须要答出来的；归一化和标准化之间的关系  

7. LR可以用来处理非线性问题么？  
特征交叉，类似fm  
核逻辑回归，类似svm  
线性变换+非线性激活，类似neural network  

8. LR可以用来处理非线性问题么？  
特征交叉，类似feature mining  
核逻辑回归，类似svm   
线性变换+非线性激活，类似neural network

9. LR的优缺点？  
优点：简单，易部署，训练速度快；模型下限较高；可解释性强   
缺点：只能线性可分；数据不平衡需要人为处理；模型上限较低

10. LR 对比其他模型？
- lr和线性回归 
lr解用的极大似然，线性回归用的最小二乘   
lr用于分类，线性回归用于回归  
但两者都是广义线性回归GLM问题  
两者对非线性问题的处理能力都是欠佳的  

- lr和最大熵  
在解决二分类问题是等同的

- lr和svm  
都可分类，都是判别式模型思路  
通常都是用正则化进行规约  
模型上  
lr是交叉熵，svm是HingeLoss  
lr是全量数据拟合，svm是支持向量拟合  
lr是参数估计有参数的前提假设，svm没有  
lr依赖的是极大似然，svm依赖的是距离  

- lr和朴素贝叶斯  
如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致  
lr是判别模型，朴素贝叶斯是生成模型    
lr没有明确feature条件独立(但是不能共线性)，朴素贝叶斯要求feature条件独立  

- lr和最大熵模型  
本质没有区别  
最大熵模型在解决二分类问题就是逻辑回归  
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归  
