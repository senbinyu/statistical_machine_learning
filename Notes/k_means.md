kmeans clustering, k均值聚类，是一种无监督的学习方法。

- kmeans流程？  
预将数据分为K组，则随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。  

- kmeans初始点除了随机选取之外的方法  
1) 尽可能选择距离最远的点，具体过程：首先随机选择一个点作为第一个初始类簇中心点，然后选择距离该点最远的那个点作为第二个初始类簇中心点，然后再选择距离前两个点的最近距离最大的点作为第三个初始类簇的中心点，以此类推，直至选出K个初始类簇中心点。  
2）选用层次聚类或者Canopy算法进行初始聚类，然后利用这些类簇的中心点作为KMeans算法初始类簇中心点。常用的层次聚类算法有BIRCH和ROCK。

- kmeans算法优缺点？  
优点：对于大数据集，算法高效和可伸缩性强，它的计算复杂度在O(NKt)，N是样本总数，K是聚类的数目，t是迭代轮数。尽管达到的经常是局部最优，但已能满足聚类的需求。  
缺点：受初值和离群点的影响大，每次结果不稳定，经常达到的是局部最优，而非全局最优，无法很好解决样本数据簇分布不均衡的情况（如某类是另一类的100倍），不太适用离散分类。  

- kmeans算法的调优？  
1). 数据归一化和离群点处理。  
2). 合理选择k值。可以采用手肘法，选择不同的k值进行处理，然后挑选误差从急剧下降到平稳的那个k值作为最终选择。但手肘法不能批量化，为此用gap statistic法来批量处理，分为K簇时，对应的损失函数为D_k，E(log D_k)为log D_k的期望，一般由蒙特卡洛方法模拟产生。Gap(K) = E(logD_k)-log(D_k)，当Gap(K)取到最大时候的K即为最终选择。  

- kmeans算法和EM算法  
kmeans算法也是EM算法的一种，其中E步就是根据每个点，算出相应的期望，相当于找到离当前最近的簇；M步就是找到最优的中心点，使得损失函数（L2距离）最小。
