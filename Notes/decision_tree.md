##### 决策树的生成包括特征选择，树的构造和树的剪枝三部分。不同决策树构建的启发函数（划分标准）是不同的，如信息增益（ID3），增益率（C4.5），GINI系数（CART）

- 信息熵到信息增益？  
信息熵：信息（定义为log pk）纯度越大，信息熵越小，而差异越大，信息熵越大。  
![ent](https://user-images.githubusercontent.com/42667259/91277082-a5b3a500-e782-11ea-8136-b9481dd5778d.png)  
ID3 用的是信息增益，信息增益是全部样本的信息熵 - 某个属性（特征）对应的信息熵(下面的Dv)，由此而得到的信息增益。某属性所占的样本数目越多，其纯度越高，最终得到的信息增益越大，因此偏好于样本多的属性。  
![entGain](https://user-images.githubusercontent.com/42667259/91277086-a64c3b80-e782-11ea-8de6-41fd5219077d.png)   
所以，为了减少这种偏好可能带来的不利影响，C4.5决策树算法使用了“增益率”，增益率则会对数目少的有偏好。
![entGainRatio](https://user-images.githubusercontent.com/42667259/91277088-a6e4d200-e782-11ea-9ec2-8ed00b966afe.png)
![entGainRatio_2](https://user-images.githubusercontent.com/42667259/91277090-a6e4d200-e782-11ea-8148-40bba2d3b160.png)

- 解释Gini系数  
Gini系数是因为之前决策树如ID3, C4.5都用信息熵得来的作为属性划分标准，但里面有log函数，计算量更大，因此，引入GINI系数。它是CART树使用的判别标准，越小表明纯度越大，使用那个使得gini_index最小的属性作为最优属性进行划分。    
![gini](https://user-images.githubusercontent.com/42667259/91277564-473af680-e783-11ea-9c79-112cf417213e.png)
![gini_2](https://user-images.githubusercontent.com/42667259/91277566-47d38d00-e783-11ea-9d76-7ab8687c3cff.png)


- 简述决策树构建过程？  
构建根节点，将所有训练数据都放在根节点   
选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类  
如果子集非空，或子集容量未小于最少数量，递归上述1，2 步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止

- 决策树的优缺点   
优点：易于实现，数据准备简单，算法对数据没有强假设，对缺失值不敏感，效率高，可以解决线性及非线性问题，有特征选择的辅助功能     
缺点：类别太多不适合，对连续性的字段比较难预测，处理特征关联性强的数据时表现不好，单颗树容易过拟合。可以通过剪枝来尽量预防过拟合，以及后续的随机森林也可减轻了过拟合。  

- 单颗决策树的剪枝？  
剪枝通常有两种，预剪枝和后剪枝。
预剪枝核心思想是在结点扩展以前，计算当前划分是否能带来模型泛化的提升，如果不能，则停止继续生长。具有思想直接，算法简单，效率高等优点，但如何准确判断何时停止，需要一定的经验。  
后剪枝核心思想是先让算法生成一颗完整的树，然后自底向上剪枝。后剪枝同样可以用减值后是否提升性能判断，但计算量更大，但通常相比预剪枝可以得到泛化能力更强的树。

- 随机森林  
随机森林是bagging的一种方法，由m颗决策树共同决策，权重相等，每颗决策树相当于一个小领域的专家。

- 随机森林的生成过程         
1）从样本集中通过重采样的方式产生n个样本。2）假设样本特征数目为K，对n个样本选择K中的k个特征，用建立决策树的方式获得最佳分割点。3）重复m次，产生m棵决策树。4）多数投票机制进行预测。


