此处先简单介绍PCA, LDA, SVD方法。

- 怎么简单使用 PCA 来划分数据且可视化呢？  
PCA，全称为 Principal Component Analysis，也就是主成分分析方法，是一种降维算法，其功能就是把 N 维的特征，通过转换映射到 K 维上（K<N），这些由原先 N 维的投射后的 K 个正交特征，就被称为主成分

- 怎么简单使用 LDA 来划分数据且可视化呢？   
LDA 的全称为 Linear Discriminant Analysis, 中文为线性判别分析，LDA 是一种有监督学习的算法，和 PCA 不同。PCA 是无监督算法，。LDA 是“投影后类内方差最小，类间方差最大”，也就是将数据投影到低维度上，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。  

- PCA, SVD, LDA区别与联系   
答：他们都是降维的方法，可以对矩阵进行压缩。  
过程上是不同的：https://zhuanlan.zhihu.com/p/74245918   
PCA: 是原先矩阵A的协方差矩阵(A-\bat{A})(A-\bat{A})^T的分解，某个基向量上的投影长度即为特征值大小，长度越大，特征值越大，即在该方向上的方差越大，也就是说选择该方向很有效。如下图所示，沿lambda2方向很好，所以取其为第一特征。  
SVD：是A的直接UV分解，一般可以通过迭代的方式求解，计算更快些   
PCA是将协方差矩阵主要的特征向量作为基向量，对A进行投影，达到降维的目的；SVD是保留大的特征值和特征向量，用秩逼近的方法来降维，虽然思想不一样，但殊途同归   
![pca_lda](https://user-images.githubusercontent.com/42667259/91284380-1ca16b80-e78c-11ea-80bf-1c1b902c3cdd.jpg)  
以上两者是无监督学习的一种，而LDA则是有监督学习，既可以作为降维，也可以作为分类的方法。最大化类间距离，最小化类内距离。定义目标函数，分子为类间距离，分母为类内距离，最大化目标函数即可得投影矩阵，然后乘以原数据即可得到投影后结果。上图中也是相当于二维的数据投影。  
![LDA_1](https://user-images.githubusercontent.com/42667259/91284377-1c08d500-e78c-11ea-9635-d7409d6ffaa6.jpg)
![LDA_2](https://user-images.githubusercontent.com/42667259/91284379-1ca16b80-e78c-11ea-9e46-e95493b56859.jpg)


